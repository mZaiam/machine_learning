{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe2c7333-c57d-49f1-becf-6a1fea5c6709",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Modelos 04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5c55b95-02c5-49c1-afd0-b9443c8ce1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19e1606-10eb-421f-b958-86f1540d9c18",
   "metadata": {},
   "source": [
    "The objective of this task is to discuss the following hypotheses:\n",
    "\n",
    "1. Performing standard normalization of the features improves the performance of models induced by the k-nearest neighbors algorithm.\n",
    "\n",
    "2. Reducing the dimensionality of the problem using PCA improves the performance of models induced by the k-nearest neighbors algorithm.\n",
    "\n",
    "3. Performing standard normalization of the features improves the performance of models induced by the decision tree algorithm.\n",
    "\n",
    "4. Reducing the dimensionality of the problem using PCA improves the performance of models induced by the decision tree algorithm.\n",
    "\n",
    "Since our goal is not to find the best model in each case but rather to compare the effect of PCA and standard normalization on the models, we will not perform hyperparameter optimization in this task and will only use the default values from `sklearn`. Next, we will test these hypotheses on the k-NN and decision tree algorithms.\n",
    "\n",
    "For hypothesis testing, we will perform cross-validation with 100 folds to represent our simulation. We will use Welch's $t$-test to compare the mean of the models before and after the transformations. Given the specifications of the test, our hypotheses will be:\n",
    "\n",
    "- $H_0$: The average RMSE of the standard model is equal to the average RMSE of the model after the transformation, meaning the transformations do not alter (or improve) its performance;\n",
    "\n",
    "- $H_1$: The average RMSE of the standard model is lower than the average RMSE of the model after the transformation, meaning the transformations worsen its performance.\n",
    "\n",
    "Defining our trust level, and our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "939400e6-5884-4e43-9807-e5ed5a0d4721",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FEATURES = ['carat', 'depth', 'table']\n",
    "TARGET = ['price']\n",
    "\n",
    "df = sns.load_dataset(\"diamonds\")\n",
    "df = df.reindex(FEATURES + TARGET, axis=1)\n",
    "df = df.dropna()  \n",
    "\n",
    "x = df.reindex(FEATURES, axis=1)\n",
    "y = df.reindex(TARGET, axis=1)\n",
    "\n",
    "x = x.values\n",
    "y = y.values.ravel()\n",
    "\n",
    "SEED = 8163295\n",
    "\n",
    "TRUST = 0.9\n",
    "SIGNIFICANCE = 1 - TRUST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322a93e3-09e7-4884-97a9-f5c564c9ad79",
   "metadata": {},
   "source": [
    "## k-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e40b19b-b2fa-4d42-baba-4120d57de113",
   "metadata": {},
   "source": [
    "First, let's train a k-NN model without applying standard normalization or PCA. We will consider the number of neighbors to be 5 for all 3 models, as this is the default value in the `sklearn` function. Instantiating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc05a339-bbe5-4ae9-912f-ef9da4d7cb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7f8b27-53d4-4a62-a200-6836ac0c1eff",
   "metadata": {},
   "source": [
    "Performing cross validation to get the RMSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4611cfd-1ea1-417b-b488-244541bb15e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The k-NN model achieved an average RMSE of 1194.1\n"
     ]
    }
   ],
   "source": [
    "NUM_FOLDS = 100\n",
    "\n",
    "rmse_knn = cross_val_score(\n",
    "    knn,\n",
    "    x,\n",
    "    y,\n",
    "    cv=NUM_FOLDS,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    ")\n",
    "\n",
    "print(f'The k-NN model achieved an average RMSE of {-rmse_knn.mean():.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bf1dbc-b2c3-4eb9-bac7-24e3083ad520",
   "metadata": {},
   "source": [
    "### k-NN with Standard Normalization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ecccc-c883-4e52-ad4a-caad76e82476",
   "metadata": {},
   "source": [
    "Now, let's use standard normalization before training the model. We will use the `make_pipeline` function to simplify the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "877eb2f2-fc4e-459c-a5a3-5113561c0e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_norm = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    KNeighborsRegressor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b79eb72-2249-43f5-8ee0-ddf61abccb39",
   "metadata": {},
   "source": [
    "Calculating the RMSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a22bb2d4-a034-49cb-8161-034c4fd6390b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The k-NN model with standard normalization achieved an average RMSE of 1171.7\n"
     ]
    }
   ],
   "source": [
    "rmse_knn_norm = cross_val_score(\n",
    "    knn_norm,\n",
    "    x,\n",
    "    y,\n",
    "    cv=NUM_FOLDS,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    ")\n",
    "\n",
    "print(f'The k-NN model with standard normalization achieved an average RMSE of {-rmse_knn_norm.mean():.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fec989-a40b-41ba-90d9-73c675befed0",
   "metadata": {},
   "source": [
    "### k-NN with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946ed427-d021-4d48-a72a-a638e0fb3c38",
   "metadata": {},
   "source": [
    "Now, let's apply PCA before training the k-NN model. We will consider only the two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e3ea8ca-d41d-48ba-b8f9-c94bb67c08e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCS = 2\n",
    "\n",
    "knn_pca = make_pipeline(\n",
    "    PCA(n_components=PCS),\n",
    "    KNeighborsRegressor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e4014a-1f84-4966-a4d2-dc86447ee761",
   "metadata": {},
   "source": [
    "Calculating the RMSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4813d04b-fed6-4f23-9e8e-a74ef27797cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The k-NN model with PCA achieved an average RMSE of 1661.4\n"
     ]
    }
   ],
   "source": [
    "rmse_knn_pca = cross_val_score(\n",
    "    knn_pca,\n",
    "    x,\n",
    "    y,\n",
    "    cv=NUM_FOLDS,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    ")\n",
    "\n",
    "print(f'The k-NN model with PCA achieved an average RMSE of {-rmse_knn_pca.mean():.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449c6007-b43e-4d17-b9cf-6019ab596d59",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12d4918-db30-4277-8f80-e5e2e5265c84",
   "metadata": {},
   "source": [
    "Let's perform the same process we did with the k-NN algorithm for the decision tree, using the default hyperparameters of the `sklearn` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61d2bdd6-a5f6-4dcc-9753-221ff9416099",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeRegressor(random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c6b874c-b080-426e-a83e-8ab8ca3aeba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The decision tree model achieved an average RMSE of 1352.5\n"
     ]
    }
   ],
   "source": [
    "rmse_tree = cross_val_score(\n",
    "    tree,\n",
    "    x,\n",
    "    y,\n",
    "    cv=NUM_FOLDS,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    ")\n",
    "\n",
    "print(f'The decision tree model achieved an average RMSE of {-rmse_tree.mean():.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53677c1c-f721-42d4-a267-5bbed210db14",
   "metadata": {},
   "source": [
    "### Decision Tree with Standard Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56a69344-403e-4a01-a08d-4d1e3964ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_norm = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    DecisionTreeRegressor(random_state=SEED)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9ce034b-0f53-4ee9-9fc4-fdf47b619681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The decision tree model with standard normalization achieved an average RMSE of 1353.1\n"
     ]
    }
   ],
   "source": [
    "rmse_tree_norm = cross_val_score(\n",
    "    tree_norm,\n",
    "    x,\n",
    "    y,\n",
    "    cv=NUM_FOLDS,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    ")\n",
    "\n",
    "print(f'The decision tree model with standard normalization achieved an average RMSE of {-rmse_tree_norm.mean():.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5884a45-5c20-43df-b4bc-a70deac7f787",
   "metadata": {},
   "source": [
    "### Decision Tree with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d41d19a-921f-483b-bfae-4a34bd62f4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_pca = make_pipeline(\n",
    "    PCA(n_components=PCS),\n",
    "    DecisionTreeRegressor(random_state=SEED)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbb1c93e-eeeb-4e39-b94b-0783754bf36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The decision tree model with PCA achieved an average RMSE of 1980.4\n"
     ]
    }
   ],
   "source": [
    "rmse_tree_pca = cross_val_score(\n",
    "    tree_pca,\n",
    "    x,\n",
    "    y,\n",
    "    cv=NUM_FOLDS,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    ")\n",
    "\n",
    "print(f'The decision tree model with PCA achieved an average RMSE of {-rmse_tree_pca.mean():.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeec71c-4168-4b10-ad34-9fd9a1034751",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a9e1be-cc06-42e1-9919-c166a55c24b8",
   "metadata": {},
   "source": [
    "With the RMSE data, let's apply Welch's $t$-test to the obtained results and determine if the hypotheses are rejected or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09d9f56e-e974-4946-a138-47ddd533c7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reject(pvalue, significance):\n",
    "    if pvalue < significance:\n",
    "        return 'With this test, we should reject H0'\n",
    "    else:\n",
    "        return 'With this test, we should not reject H0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc024c1d-6112-4060-8db5-8d5d4d3248ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_comparison(algorithm, rmse, transformation, rmse_transformation):\n",
    "    ''' Returns if the model improved its average performance after a transformation, with respect \n",
    "    to the RMSE metric '''\n",
    "    \n",
    "    rmse_mean = -rmse.mean()\n",
    "    rmse_transformation_mean = -rmse_transformation.mean()\n",
    "    \n",
    "    if rmse_mean > rmse_transformation_mean:\n",
    "        return f'The {algorithm} algorithm improved, on average, its performance after the {transformation} transformation by {rmse_mean - rmse_transformation_mean}'\n",
    "    else:\n",
    "        return f'The {algorithm} algorithm worsened, on average, its performance after the {transformation} transformation by {-rmse_mean + rmse_transformation_mean}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a082031a-c8a1-4ae8-ba0d-8e816b6cb266",
   "metadata": {},
   "source": [
    "### Hypothesis 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a8d51da-95a6-4d99-b99e-36625bdf094f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The k-NN algorithm improved, on average, its performance after the standard normalization transformation by 22.36081972982356\n"
     ]
    }
   ],
   "source": [
    "print(rmse_comparison('k-NN', rmse_knn, 'standard normalization', rmse_knn_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0409e15c-b257-4aa1-a26b-4919b8a627e6",
   "metadata": {},
   "source": [
    "Standard normalization of the features improved, on average, the k-NN algorithm. Let's see if this hypothesis is confirmed by Welch's test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "868fc2c8-e5a9-4e26-a509-d6dc74add59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With this test, we should not reject H0, given the p-value of 0.562.\n"
     ]
    }
   ],
   "source": [
    "test_knn_norm = stats.ttest_ind(-rmse_knn, -rmse_knn_norm, equal_var=False, alternative='less')\n",
    "\n",
    "pvalue_knn_norm = test_knn_norm.pvalue\n",
    "\n",
    "print(f'{reject(pvalue_knn_norm, SIGNIFICANCE)}, given the p-value of {pvalue_knn_norm:.3f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8641a7e4-f050-4295-9833-0cd2f2c81125",
   "metadata": {},
   "source": [
    "Indeed, we should not reject the fact that standard normalization of the features improves the performance of the k-NN model in this case.\n",
    "\n",
    "The k-NN model is a distance-based algorithm, meaning the intuition behind it is that similar results occupy nearby regions in the space, according to some predefined mathematical distance. If there is a disparity in the magnitude of the features, the distance calculation will be influenced by features with higher absolute values, which can hinder the model’s prediction (since these may not necessarily be the most important attributes for predicting the target).\n",
    "\n",
    "When all features are normalized, the magnitude of the features does not influence the distance calculation, making the algorithm more effective in prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d402fc5-c90f-4a12-9a20-b0c6111ab02e",
   "metadata": {},
   "source": [
    "### Hypothesis 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae010137-a62c-4ea3-9ce8-ebe5c78d4bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The k-NN algorithm worsened, on average, its performance after the PCA transformation by 467.28301504075307\n"
     ]
    }
   ],
   "source": [
    "print(rmse_comparison('k-NN', rmse_knn, 'PCA', rmse_knn_pca))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ee6b58-61bc-4a82-a32d-64ae09aacc51",
   "metadata": {},
   "source": [
    "As we can see, the algorithm worsened, on average, after PCA. Let's check if this hypothesis is indeed rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef1eba8f-7ac5-4edc-a66d-bae669ee0716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With this test, we should reject H0, given the p-value of 0.001\n"
     ]
    }
   ],
   "source": [
    "test_knn_pca = stats.ttest_ind(-rmse_knn, -rmse_knn_pca, equal_var=False, alternative='less')\n",
    "\n",
    "pvalue_knn_pca = test_knn_pca.pvalue\n",
    "\n",
    "print(f'{reject(pvalue_knn_pca, SIGNIFICANCE)}, given the p-value of {pvalue_knn_pca:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0bc75c-1cc7-4067-b544-2dc80a5105bd",
   "metadata": {},
   "source": [
    "We should reject, in this case, that PCA improves the k-NN algorithm.\n",
    "\n",
    "After PCA, we changed the distances between the original points, causing the distances between them to also change. Since k-NN relies on the intuition that the distance between points directly reflects their target values, altering the spatial orientation of the features might remove the relationship between the original distances of the data and their similarities. This could be why PCA was not effective in this particular case. Additionally, since we had only 3 dimensions and reduced them to 2 with PCA, we might have lost a lot of useful information in the process, something that might not have happened if we had a significantly larger number of dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ac9ff1-7c57-4809-84bc-c12ebc6bbab5",
   "metadata": {},
   "source": [
    "### Hypothesis 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "368943bd-272c-4044-9c1d-7bd146063afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The decision tree algorithm worsened, on average, its performance after the standard normalization transformation by 0.598185748386868\n"
     ]
    }
   ],
   "source": [
    "print(rmse_comparison('decision tree', rmse_tree, 'standard normalization', rmse_tree_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90a8cab-4637-413a-8ee2-b894cf865ea5",
   "metadata": {},
   "source": [
    "As we can see, the decision tree algorithm worsened slightly after applying standard normalization. Let's check if this hypothesis is rejected or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "809b120d-e443-43ca-9221-949856fb7746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With this test, we should not reject H0, given the p-value of 0.499\n"
     ]
    }
   ],
   "source": [
    "test_tree_norm = stats.ttest_ind(-rmse_tree, -rmse_tree_norm, equal_var=False, alternative='less')\n",
    "\n",
    "pvalue_tree_norm = test_tree_norm.pvalue\n",
    "\n",
    "print(f'{reject(pvalue_tree_norm, SIGNIFICANCE)}, given the p-value of {pvalue_tree_norm:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347c1e38-ef2c-4290-8d69-1148c7235033",
   "metadata": {},
   "source": [
    "As we can see, we should not reject, in this case, that standard normalization improved the decision tree model. \n",
    "\n",
    "The decision tree model is a search-based algorithm with the intuition of breaking down a larger problem into smaller problems. During its training, the tree divides the initial set into subspaces (according to the impurity function of a split node), thus simplifying the prediction. Consequently, the tree does not care about the magnitude or spread of the data, only about the split point that generates the least impurity in the subsequent nodes. Therefore, any transformation that does not alter the order of the data will not affect the decision tree formation process. This is why we observe such a small improvement in the RMSE of trees with standard normalization, merely due to the random process of cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda98f5c-b34e-4461-b3a6-5ebbfb73c1b3",
   "metadata": {},
   "source": [
    "We can see that normalization makes little difference when training a decision tree with all the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9bbb0c0c-264f-4d4a-a596-395b2e843e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE of the decision tree without normalization was 1599.5, and with normalization was 1603.0\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "                    x, y, test_size=0.1, random_state=SEED)\n",
    "\n",
    "tree_total = DecisionTreeRegressor(random_state=SEED)\n",
    "\n",
    "tree_total.fit(x_train, y_train)\n",
    "\n",
    "y_tree_total = tree_total.predict(x_test)\n",
    "\n",
    "tree_norm_total = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    DecisionTreeRegressor(random_state=SEED)\n",
    ")\n",
    "\n",
    "tree_norm_total.fit(x_train, y_train)\n",
    "\n",
    "y_tree_norm_total = tree_norm_total.predict(x_test)\n",
    "\n",
    "print(f'The RMSE of the decision tree without normalization was {root_mean_squared_error(y_tree_total, y_test):.1f}, and with normalization was {root_mean_squared_error(y_tree_norm_total, y_test):.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a009eea-ec7b-42c8-9047-2dd0aa1514d7",
   "metadata": {},
   "source": [
    "### Hypothesis 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f185acd-a9a2-4578-a308-9abf7d2aaa61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The decision tree algorithm worsened, on average, its performance after the PCA transformation by 627.908433802101\n"
     ]
    }
   ],
   "source": [
    "print(rmse_comparison('decision tree', rmse_tree, 'PCA', rmse_tree_pca))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6986d0f7-4bf4-42b3-b7b3-a699fc90a884",
   "metadata": {},
   "source": [
    "We can see that, on average, decision trees worsened significantly after PCA. Performing the hypothesis test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d67762fe-4d64-4128-9e24-5a471dc4ce78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With this test, we should reject H0, given the p-value of 0.000\n"
     ]
    }
   ],
   "source": [
    "test_tree_pca = stats.ttest_ind(-rmse_tree, -rmse_tree_pca, equal_var=False, alternative='less')\n",
    "\n",
    "pvalue_tree_pca = test_tree_pca.pvalue\n",
    "\n",
    "print(f'{reject(pvalue_tree_pca, SIGNIFICANCE)}, given the p-value of {pvalue_tree_pca:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec046727-709c-4aa0-8e4e-9c5d18f290a3",
   "metadata": {},
   "source": [
    "This was the most unexpected result among the 4 hypotheses. As we mentioned, the decision tree relies on the information gain at each decision node, and we believed that after performing PCA, the tree would have an easier time making splits, as it would be analyzing the components with the highest variance of the observed data. However, the algorithm performed significantly worse, possibly because too much information was lost by excluding a principal component.\n",
    "\n",
    "Let's quickly analyze this new hypothesis by training two trees with all the training data and applying PCA, but without excluding any principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9276248e-3a12-4d14-a3c0-15c29ebf6ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE of the decision tree without normalization was 1599.5, and with PCA was 1597.1\n"
     ]
    }
   ],
   "source": [
    "tree_pca_total = make_pipeline(\n",
    "    PCA(),\n",
    "    DecisionTreeRegressor(random_state=SEED)\n",
    ")\n",
    "\n",
    "tree_pca_total.fit(x_train, y_train)\n",
    "\n",
    "y_tree_pca_total = tree_pca_total.predict(x_test)\n",
    "\n",
    "print(f'The RMSE of the decision tree without normalization was {root_mean_squared_error(y_tree_total, y_test):.1f}, and with PCA was {root_mean_squared_error(y_tree_pca_total, y_test):.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf1435c-f109-441e-a46e-27f49f37c720",
   "metadata": {},
   "source": [
    "As we can see, the performance improved slightly. Now, excluding one principal component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98f0e4dc-22f8-4a2a-aef6-6ee48ddf6bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE of the decision tree without normalization was 1599.5, and with PCA excluding 1 PC was 2137.4\n"
     ]
    }
   ],
   "source": [
    "tree_pca2_total = make_pipeline(\n",
    "    PCA(n_components=2),\n",
    "    DecisionTreeRegressor(random_state=SEED)\n",
    ")\n",
    "\n",
    "tree_pca2_total.fit(x_train, y_train)\n",
    "\n",
    "y_tree_pca2_total = tree_pca2_total.predict(x_test)\n",
    "\n",
    "print(f'The RMSE of the decision tree without normalization was {root_mean_squared_error(y_tree_total, y_test):.1f}, and with PCA excluding 1 PC was {root_mean_squared_error(y_tree_pca2_total, y_test):.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45239d73-e393-4593-826e-8beffb5bd93e",
   "metadata": {},
   "source": [
    "In this case, we see a significant deterioration in performance. This suggests that excluding a principal component removes too much information from our data. However, in cases with many more features, we might observe an improvement in the decision tree’s performance after dimensionality reduction with PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d10a35b-9bc8-471e-b98e-0d51cf8cb544",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44d8c4e-d3a3-4136-b336-fc445ba7304f",
   "metadata": {},
   "source": [
    "After testing the 4 initial hypotheses and analyzing the performance of the k-NN and decision tree algorithms, we have gathered strong indications about the influence of standard normalization and PCA on these models. The k-NN algorithm benefits from standard normalization but may be negatively affected by PCA. In contrast, the decision tree does not change significantly with standard normalization but can experience substantial influences, both positive and negative, with the application of PCA (this is the case that should be clarified further)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ilumpy",
   "language": "python",
   "name": "ilumpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
